{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hh = np.random.standard_normal((3,2))\n",
    "w_hx = np.random.standard_normal((3,3))\n",
    "h_t_prev = np.random.standard_normal((2,1))\n",
    "x_t = np.random.standard_normal((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_1 = np.hstack((w_hh, w_hx))\n",
    "\n",
    "stack_2 = np.vstack((h_t_prev, x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.matmul(np.hstack((w_hh, w_hx)), np.vstack((h_t_prev, x_t))))\n",
    "print(np.matmul(stack_1,stack_2))\n",
    "print(stack_2)\n",
    "print(np.concatenate([h_t_prev, x_t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from time import perf_counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)                 # Random seed, so your results match ours\n",
    "emb = 128                       # Embedding size\n",
    "T = 256                         # Length of sequence\n",
    "h_dim = 16                      # Hidden state dimension\n",
    "h_0 = np.zeros((h_dim, 1))     \n",
    " \n",
    "w1 = random.standard_normal((h_dim, emb + h_dim))\n",
    "w2 = random.standard_normal((h_dim, emb + h_dim))\n",
    "w3 = random.standard_normal((h_dim, emb + h_dim))\n",
    "\n",
    "b1 = random.standard_normal((h_dim, 1))\n",
    "b2 = random.standard_normal((h_dim, 1))\n",
    "b3 = random.standard_normal((h_dim, 1))\n",
    "\n",
    "X = random.standard_normal((T, emb, 1))\n",
    "\n",
    "weights_vanilla = [w1, b1]\n",
    "weights_GRU = [w1.copy(), w2, w3, b1.copy(), b2, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_RNN(inputs, weights):\n",
    "    x, ht = inputs\n",
    "    wh, bh = weights\n",
    "    \n",
    "    ht = np.matmul(wh, np.vstack((ht, x)))+bh\n",
    "    ht = sigmoid(ht)\n",
    "\n",
    "    y = ht\n",
    "    #print(ht)\n",
    "    return y,ht \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_GRU_RNN(inputs, weights):\n",
    "    x, ht = inputs\n",
    "    wu,wr,wh, bu,br,bh = weights\n",
    "\n",
    "    r = sigmoid(np.matmul(wr, np.vstack((ht, x)))+br)\n",
    "    u = sigmoid(np.matmul(wu, np.vstack((ht, x)))+bu)\n",
    "    ct = np.tanh(np.matmul(wh, np.concatenate([r * ht, x]))+bh)\n",
    "\n",
    "    #print(f'{u}\\n+\\n{r}\\n+\\n{ct}')\n",
    "    \n",
    "    ht = u*ct + (1-u) * ht\n",
    "    y = ht\n",
    "\n",
    "    return y,ht \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forward_GRU_RNN([X[1], h_0], weights_GRU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(function, elems, weights, initializer=h_0):\n",
    "    cur_value = initializer\n",
    "    ys = []\n",
    "    for x in elems:\n",
    "        y,cur_value = function([x,cur_value],weights)\n",
    "        ys.append(y)\n",
    "    return ys,cur_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, h_T = scan(forward_RNN, X, weights_vanilla, h_0)\n",
    "\n",
    "print(f\"Length of ys: {len(ys)}\")\n",
    "print(f\"Shape of each y within ys: {ys[0].shape}\")\n",
    "print(f\"Shape of h_T: {h_T.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "ys, h_T = scan(forward_RNN, X, weights_vanilla, h_0)\n",
    "toc = perf_counter()\n",
    "RNN_time=(toc-tic)*1000\n",
    "print (f\"It took {RNN_time:.2f}ms to run the forward method for the vanilla RNN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "ys, h_T = scan(forward_GRU_RNN, X, weights_GRU, h_0)\n",
    "toc = perf_counter()\n",
    "GRU_time=(toc-tic)*1000\n",
    "print (f\"It took {GRU_time:.2f}ms to run the forward method for the GRU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(256, return_sequences=True, name='GRU_1_returns_seq'),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, name='GRU_2_returns_seq'),\n",
    "    tf.keras.layers.GRU(64, name='GRU_3_returns_last_only'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_GRU.summary()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember these three numbers and follow them further through the notebook\n",
    "batch_size = 60\n",
    "sequence_length = 50\n",
    "word_vector_length = 40\n",
    "\n",
    "input_data = tf.random.normal([batch_size, sequence_length, word_vector_length])\n",
    "\n",
    "prediction = model_GRU(input_data)\n",
    "\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'data/'\n",
    "filename = 'shakespeare_data.txt'\n",
    "lines = [] # storing all the lines in a variable. \n",
    "\n",
    "counter = 0\n",
    "\n",
    "with open(os.path.join(dirname, filename)) as files:\n",
    "    for line in files:        \n",
    "        pure_line = line.strip()\n",
    "        if pure_line:\n",
    "            lines.append(pure_line)\n",
    "            \n",
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(\"\\n\".join(lines[506:514]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(lines):\n",
    "    corpus = (\"\\n\".join(lines))\n",
    "    vocab = sorted(set(corpus))\n",
    "    vocab.insert(0,\"[UNK]\") \n",
    "    vocab.insert(1,\"\") \n",
    "    return vocab\n",
    "vocab = build_vocabulary(lines)\n",
    "print(len(vocab))\n",
    "print(\" \".join(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_tensor(text,vocab):\n",
    "    chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\n",
    "    return  tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
    "tmp = convert_text_to_tensor(\"abc xyz\", vocab)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_to_text(tensor, vocab):\n",
    "    chars_from_ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None, invert=True)\n",
    "    return tf.strings.reduce_join(chars_from_ids(tensor), axis=-1).numpy()\n",
    "print(convert_tensor_to_text(tmp, vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = lines[:-1000]\n",
    "eval_lines = lines[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "print(test_train_split(list(\"Tensorflow\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(vocab, lines, seq_length=100, batch_size=64, BUFFER_SIZE = 10000):\n",
    "\n",
    "    line  = \"\\n\".join(lines)\n",
    "    all_ids = convert_text_to_tensor(line, vocab)\n",
    "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "    data_generator = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "    dataset_xy = data_generator.map(test_train_split)\n",
    "    dataset = (                                   \n",
    "        dataset_xy                                \n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(batch_size, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)  \n",
    "        )            \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = generate_dataset(vocab, train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size, activation=tf.nn.log_softmax)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "vocab_size = 82  # Adjust as needed\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "model = create_gru_model(vocab_size, embedding_dim, rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=(None, 100))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(\"Input: \", input_example_batch[0].numpy()) # Lets use only the first sequence on the batch\n",
    "    example_batch_predictions = model(tf.constant([input_example_batch[0].numpy()]))\n",
    "    print(\"\\n\",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch_predictions[0][99].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.math.argmax(example_batch_predictions[0], axis=1)\n",
    "print(sampled_indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input:\\n\", convert_tensor_to_text(input_example_batch[0], vocab))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", convert_tensor_to_text(sampled_indices, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.00125)\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 30\n",
    "model = compile_model(model)\n",
    "history = model.fit(dataset, epochs = Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"saved.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(\"Input: \", input_example_batch[0].numpy()) # Lets use only the first sequence on the batch\n",
    "    example_batch_predictions = model(tf.constant([input_example_batch[0].numpy()]))\n",
    "    print(\"\\n\",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.math.argmax(example_batch_predictions[0], axis=1)\n",
    "print(sampled_indices.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input:\\n\", convert_tensor_to_text(input_example_batch[0], vocab))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", convert_tensor_to_text(sampled_indices, vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
