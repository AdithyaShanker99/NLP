{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'SentAnalysisNN.ipynb']\n",
      "c:\\Users\\Adi\\Documents\\GitHub\\NLP\\Course3\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(os.listdir())\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Using 4000 files for training.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Using 1000 files for validation.\n",
      "Found 5000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_training_set = tf.keras.utils.text_dataset_from_directory(\n",
    "    f'{cwd}/{data_dir}/train',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=32, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Create the validation set. Use 20% of the data that was not used for training.\n",
    "raw_validation_set = tf.keras.utils.text_dataset_from_directory(\n",
    "    f'{cwd}/{data_dir}/train',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=32, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Create the test set.\n",
    "raw_test_set = tf.keras.utils.text_dataset_from_directory(\n",
    "    f'{cwd}/{data_dir}/test',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:\n",
      " b'This is a reunion, a team, and a great episode of Justice. From hesitation to resolution, Clark has made a important leap from a troubled teenager who was afraid of a controlled destiny, to a Superman who, like Green Arrow, sets aside his emotions to his few loved ones, ready to save the whole planet. This is not just a thrilling story about teamwork, loyalty, and friendship; this is also about deciding what\\'s more important in life, a lesson for Clark. I do not want the series to end, but I hope the ensuing episodes will strictly stick to what Justice shows without any \"rewind\" pushes and put a good end here of Smallville---and a wonderful beginning of Superman.<br /><br />In this episode, however, we should have seen more contrast between Lex and the Team. Nine stars should give it enough credit.'\n",
      "Label: 1\n",
      "\n",
      "Review:\n",
      " b'\"Hey Babu Riba\" is a film about a young woman, Mariana (nicknamed \"Esther\" after a famous American movie star), and four young men, Glenn, Sacha, Kicha, and Pop, all perhaps 15-17 years old in 1953 Belgrade, Yugoslavia. The five are committed friends and crazy about jazz, blue jeans, or anything American it seems.<br /><br />The very close relationship of the teenagers is poignant, and ultimately a sacrifice is willingly made to try to help one of the group who has fallen on unexpected difficulties. In the wake of changing communist politics, they go their separate ways and reunite in 1985 (the year before the film was made).<br /><br />I enjoyed the film with some reservations. The subtitles for one thing were difficult. Especially in the beginning, there were a number of dialogues which had no subtitles at all. Perhaps the conversational pace required it, but I couldn\\'t always both read the text and absorb the scene, which caused me to not always understand which character was involved. I watched the movie (a video from our public library) with a friend, and neither of us really understood part of the story about acquiring streptomycin for a sick relative.<br /><br />This Yugoslavian coming of age film effectively conveyed the teenagers\\' sense of invulnerability, idealism, and strong and loyal bonds to each other. There is a main flashforward, and it was intriguing, keeping me guessing until the end as to who these characters were vis-a-vis the 1953 cast, and what had actually happened.<br /><br />I would rate it 7 out of 10, and would like to see other films by the director, Jovan Acin (1941-1991).'\n",
      "Label: 1\n",
      "\n",
      "Review:\n",
      " b\"No message. No symbolism. No dark undercurrents.Just a wonderful melange of music, nostalgia and good fun put to-gether by people who obviously had a great time doing it. It's a refreshing antidote to some of the pretentious garbage being ground out by the studios. Of course ANYTHING with the incomparable Judi Dench is worth watching. And Cleo Laine's brilliant jazz singing is a bonus. This lady is in the same league as the late Ella. This goes on my movie shelf to be pulled out again anytime I feel the need for a warm experience and a hearty good natured chuckle. Just a wonderful film!\"\n",
      "Label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_training_set.take(1):\n",
    "    for i in range(3):\n",
    "        print(f\"Review:\\n {text_batch.numpy()[i]}\")\n",
    "        print(f\"Label: {label_batch.numpy()[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(data):\n",
    "    lower = tf.strings.lower(data)\n",
    "    strip_html = tf.strings.regex_replace(lower, '<br />', ' ')\n",
    "    replaced = tf.strings.regex_replace(\n",
    "            strip_html,\n",
    "            '[%s]' % re.escape(string.punctuation),\n",
    "            ''\n",
    "        )    \n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=10000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "# grab just the text value from the list of text and labels\n",
    "train_text = raw_training_set.map(lambda x, y: x)\n",
    "# build vocabulary\n",
    "text_layer.adapt(train_text)\n",
    "# Print out the vocabulary size\n",
    "print(f\"Vocabulary size: {len(text_layer.get_vocabulary())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:\n",
      "b\"This movie is about a side of Ireland that Americans don't normally see, the narrow-minded religiously prejudiced side of the 'friendliest race in the world'. The movie, by the admission of the inhabitants of Fethard who are old enough to remember the events, is fairly accurate (though they insist that the film-makers invented some of the more violent scenes just to spice up the action).<br /><br />The movie was very unpopular in Ireland as it portrayed the Catholic church in a bad light, but the simple fact is that representatives of the Catholic church *did* organise vetoes of minorities (before Protestants it was the Jews).<br /><br />The film is a fascinating insight into the whole issue of religion in Ireland\"\n",
      "\n",
      "Label: pos\n",
      "\n",
      "Vectorized review\n",
      "(<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[  10,   17,    7,   42,    3,  421,    5, 2801,   12, 1931,   87,\n",
      "        1709,   65,    2,    1,    1,    1,  421,    5,    2,    1, 1487,\n",
      "           8,    2,  185,    2,   17,   32,    2, 6940,    5,    2, 6172,\n",
      "           5,    1,   35,   24,  180,  181,    6,  352,    2,  638,    7,\n",
      "        1050, 2012,  148,   34, 7995,   12,    2,  850, 5067,   47,    5,\n",
      "           2,   50,  911,  132,   41,    6, 6499,   57,    2,  224,    2,\n",
      "          17,   13,   52,    1,    8, 2801,   14,    9,  898,    2, 3014,\n",
      "        1381,    8,    3,   81,  733,   18,    2,  631,  199,    7,   12,\n",
      "           1,    5,    2, 3014, 1381,  116,    1,    1,    5,    1,  147,\n",
      "           1,    9,   13,    2, 2977,    2,   19,    7,    3, 1248, 2255,\n",
      "          77,    2,  211, 2143,    5, 2238,    8, 2801,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_text(text, label):\n",
    "   \n",
    "    text = tf.expand_dims(text, -1)\n",
    "    \n",
    "    vectorized_text = text_layer(text)\n",
    "    \n",
    "    # Return the vectorized text along with the label.\n",
    "    return vectorized_text, label\n",
    "\n",
    "# Get one batch and select the first datapoint\n",
    "text_batch, label_batch = next(iter(raw_training_set))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "\n",
    "# Show the raw data\n",
    "print(f\"Review:\\n{first_review}\")\n",
    "print(f\"\\nLabel: {raw_training_set.class_names[first_label]}\")\n",
    "# Show the vectorized data\n",
    "print(f\"\\nVectorized review\\n{vectorize_text(first_review, first_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_training_set.map(vectorize_text)\n",
    "val_ds = raw_validation_set.map(vectorize_text)\n",
    "test_ds = raw_test_set.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "# Create the model by calling tf.keras.Sequential, where the layers are given in a list.\n",
    "model_sequential = tf.keras.Sequential([\n",
    "    layers.Embedding(input_dim=10000, output_dim=16),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_sequential.compile(loss=losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print out the summary of the model\n",
    "model_sequential.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_sequential # model = model_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "125/125 - 2s - 16ms/step - accuracy: 0.5587 - loss: 0.6896 - val_accuracy: 0.5930 - val_loss: 0.6838\n",
      "Epoch 2/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.6305 - loss: 0.6764 - val_accuracy: 0.6520 - val_loss: 0.6691\n",
      "Epoch 3/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.6858 - loss: 0.6562 - val_accuracy: 0.6980 - val_loss: 0.6464\n",
      "Epoch 4/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.7240 - loss: 0.6273 - val_accuracy: 0.7140 - val_loss: 0.6187\n",
      "Epoch 5/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.7525 - loss: 0.5926 - val_accuracy: 0.7460 - val_loss: 0.5881\n",
      "Epoch 6/25\n",
      "125/125 - 0s - 2ms/step - accuracy: 0.7795 - loss: 0.5553 - val_accuracy: 0.7730 - val_loss: 0.5575\n",
      "Epoch 7/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.8105 - loss: 0.5178 - val_accuracy: 0.7940 - val_loss: 0.5291\n",
      "Epoch 8/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.8382 - loss: 0.4820 - val_accuracy: 0.8060 - val_loss: 0.5036\n",
      "Epoch 9/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.8575 - loss: 0.4489 - val_accuracy: 0.8150 - val_loss: 0.4811\n",
      "Epoch 10/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.8698 - loss: 0.4188 - val_accuracy: 0.8230 - val_loss: 0.4615\n",
      "Epoch 11/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.8798 - loss: 0.3917 - val_accuracy: 0.8310 - val_loss: 0.4445\n",
      "Epoch 12/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.8888 - loss: 0.3672 - val_accuracy: 0.8370 - val_loss: 0.4298\n",
      "Epoch 13/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.8972 - loss: 0.3449 - val_accuracy: 0.8400 - val_loss: 0.4170\n",
      "Epoch 14/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9030 - loss: 0.3247 - val_accuracy: 0.8440 - val_loss: 0.4058\n",
      "Epoch 15/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9070 - loss: 0.3061 - val_accuracy: 0.8420 - val_loss: 0.3961\n",
      "Epoch 16/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9115 - loss: 0.2891 - val_accuracy: 0.8440 - val_loss: 0.3877\n",
      "Epoch 17/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9210 - loss: 0.2733 - val_accuracy: 0.8490 - val_loss: 0.3803\n",
      "Epoch 18/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9285 - loss: 0.2586 - val_accuracy: 0.8530 - val_loss: 0.3738\n",
      "Epoch 19/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9345 - loss: 0.2448 - val_accuracy: 0.8580 - val_loss: 0.3681\n",
      "Epoch 20/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9410 - loss: 0.2320 - val_accuracy: 0.8580 - val_loss: 0.3632\n",
      "Epoch 21/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9445 - loss: 0.2199 - val_accuracy: 0.8560 - val_loss: 0.3589\n",
      "Epoch 22/25\n",
      "125/125 - 0s - 2ms/step - accuracy: 0.9490 - loss: 0.2086 - val_accuracy: 0.8580 - val_loss: 0.3551\n",
      "Epoch 23/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9542 - loss: 0.1978 - val_accuracy: 0.8600 - val_loss: 0.3518\n",
      "Epoch 24/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9560 - loss: 0.1877 - val_accuracy: 0.8570 - val_loss: 0.3490\n",
      "Epoch 25/25\n",
      "125/125 - 0s - 3ms/step - accuracy: 0.9597 - loss: 0.1781 - val_accuracy: 0.8560 - val_loss: 0.3466\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8396 - loss: 0.3724\n",
      "Loss: 0.3683178424835205\n",
      "Accuracy: 0.8407999873161316\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new sequential model using the vectorization layer and the model you just trained.\n",
    "export_model = tf.keras.Sequential([\n",
    "  text_layer,\n",
    "  model]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 0.588,   Label: 1,   Review: excellent\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of strings to a tensor and expand its dimensions\n",
    "examples = ['excellent']\n",
    "examples_tensor = tf.constant(examples)\n",
    "\n",
    "# Make predictions using the model\n",
    "results = export_model.predict(examples_tensor, verbose=False)\n",
    "for result, example in zip(results, examples):\n",
    "    print(f'Result: {result[0]:.3f},   Label: {int(np.round(result[0]))},   Review: {example}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58751833]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
