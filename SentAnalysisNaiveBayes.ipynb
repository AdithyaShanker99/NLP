{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Adi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Adi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from nltk.corpus import twitter_samples\n",
    "import random\n",
    "import numpy as np\n",
    "import math as m\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use predefined function ‘fileids()’ to see the content\n",
    "twitter_samples.fileids()\n",
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGreat to have him as part of the awesome #Dominion family :) https://t.co/sLmo2Voxzx\n",
      "\u001b[91mi’m so sad i want my hair colour back now :(\n"
     ]
    }
   ],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "import re\n",
    "import string                              # for string operations\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "tweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \n",
    "labels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n",
    "\n",
    "train_pos  = all_positive_tweets[:4000]\n",
    "train_neg  = all_negative_tweets[:4000]\n",
    "train_x = train_pos + train_neg \n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)), axis=0).reshape(-1)\n",
    "\n",
    "\n",
    "test_pos  = all_positive_tweets[1000:]\n",
    "test_neg  = all_negative_tweets[1000:]\n",
    "test_x = test_pos + test_neg\n",
    "test_y = np.append(np.ones(len(test_pos)),np.zeros(len(test_neg)), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet) :\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    cleaned_stemmed_tweet = []\n",
    "    for token in tweet_tokens:\n",
    "        if token not in punctuation :\n",
    "            cleaned_stemmed_tweet.append(token)\n",
    "    return cleaned_stemmed_tweet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def buildFrequencyTable(features_x,labels_list):\n",
    "    dictionary = {}\n",
    "    sum_pos = 0\n",
    "    sum_neg = 0\n",
    "    for i in range(len(features_x)) :\n",
    "        tokens = preprocess(features_x[i])\n",
    "        for token in tokens:\n",
    "            if token not in dictionary:\n",
    "                if labels_list[i] == 1:\n",
    "                    dictionary[token] = (1,0)\n",
    "                    sum_pos+=1\n",
    "                else:\n",
    "                    dictionary[token] = (0,1)\n",
    "                    sum_neg+=1\n",
    "            else:\n",
    "                if labels_list[i] == 1:\n",
    "                    dictionary[token] = (dictionary[token][0]+1,dictionary[token][1])\n",
    "                    sum_pos+=1\n",
    "                else:\n",
    "                    dictionary[token] = (dictionary[token][0], dictionary[token][1]+1)\n",
    "                    sum_neg+=1\n",
    "    return dictionary,sum_pos,sum_neg\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(dictionary,sum_pos, sum_neg, alpha):\n",
    "    for key in dictionary:\n",
    "        dictionary[key] = (dictionary[key][0]+alpha)/(sum_pos+len(dictionary)) , (dictionary[key][1]+alpha)/(sum_neg+len(dictionary))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': (3, 3), 'am': (3, 3), 'happy': (2, 1), 'because': (1, 0), 'learning': (1, 1), 'nlp': (1, 1), 'not': (1, 2), 'sad': (1, 2)}\n",
      "\n",
      "0.9999999999999999 , 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "x_temp = [\"I am happy because I am learning NLP I am happy, not sad\",\"I am sad, I am not learning NLP I am sad,not happy\"]\n",
    "temp_labels = [1,0]\n",
    "dictionary,sum_pos,sum_neg = buildFrequencyTable(x_temp,temp_labels)\n",
    "print(dictionary)\n",
    "dictionary = laplace_smoothing(dictionary,sum_pos,sum_neg,1)\n",
    "print()\n",
    "pos_test_sum = 0\n",
    "neg_test_sum = 0\n",
    "for key in dictionary:\n",
    "    pos_test_sum+=dictionary[key][0]\n",
    "    neg_test_sum+=dictionary[key][1]\n",
    "print(f'{pos_test_sum} , {neg_test_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931471805599454\n",
      "[  0.           7.67132438 -10.83586812]\n",
      "Epoch 100, Loss: 0.05380451807778632\n",
      "[ 3.60093532e-03  8.12504401e+00 -8.73426842e+00]\n",
      "Epoch 200, Loss: 0.048563505419131035\n",
      "[ 5.40354262e-03  7.37899846e+00 -7.68161702e+00]\n",
      "Epoch 300, Loss: 0.04835419074449679\n",
      "[ 0.00690303  6.49537431 -6.76040971]\n",
      "Epoch 400, Loss: 0.04824862446026552\n",
      "[ 0.00839992  5.611057   -5.83987276]\n",
      "Epoch 500, Loss: 0.04815211106119109\n",
      "[ 0.00989522  4.72672811 -4.91935457]\n",
      "Epoch 600, Loss: 0.04785769137222982\n",
      "[ 0.01138915  3.84240224 -3.99884635]\n",
      "Epoch 700, Loss: 0.04755400561889181\n",
      "[ 0.01288289  2.95809305 -3.07834452]\n",
      "Epoch 800, Loss: 0.047261201802485886\n",
      "[ 0.01437939  2.07381784 -2.15785137]\n",
      "Epoch 900, Loss: 0.046023274571399\n",
      "[ 0.01588388  1.18960283 -1.23739229]\n",
      "Epoch 1000, Loss: 0.04247813627540391\n",
      "[ 0.01741819  0.30526287 -0.31754746]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995125\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01741819  0.30526287 -0.31754746]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
