{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Adi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Adi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading tokenize: Package 'tokenize' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from nltk.corpus import twitter_samples\n",
    "import random\n",
    "import numpy as np\n",
    "import math as m\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('tokenize')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use predefined function ‘fileids()’ to see the content\n",
    "twitter_samples.fileids()\n",
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m@Mehdi_Mustafa @Ali_SyedRaza \n",
      "ROFL not going to happen, rest assured :D\n",
      "\u001b[91mI thought finding someone was hard but finding them and trying to make them stay is harder :(\n",
      "\n",
      "#imintoher\n"
     ]
    }
   ],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "import re\n",
    "import string                              # for string operations\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "tweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \n",
    "labels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n",
    "\n",
    "train_pos  = all_positive_tweets[:4000]\n",
    "train_neg  = all_negative_tweets[:4000]\n",
    "train_x = train_pos + train_neg \n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)), axis=0).reshape(-1)\n",
    "\n",
    "\n",
    "test_pos  = all_positive_tweets[1000:]\n",
    "test_neg  = all_negative_tweets[1000:]\n",
    "test_x = test_pos + test_neg\n",
    "test_y = np.append(np.ones(len(test_pos)),np.zeros(len(test_neg)), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet) :\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    cleaned_stemmed_tweet = []\n",
    "    for token in tweet_tokens:\n",
    "        if token not in stop_words and token not in punctuation :\n",
    "            cleaned_stemmed_tweet.append(stemmer.stem(token))\n",
    "    return cleaned_stemmed_tweet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'cant', 'chang', 'peopl', 'feel', 'u', 'dnt', 'tri', 'live', 'ur', 'life', 'happi', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_processed = preprocess(all_positive_tweets[1456])\n",
    "print(tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dictionary = {}\n",
    "negative_dictionary = {}\n",
    "\n",
    "for i in range(len(all_positive_tweets)) :\n",
    "    pos_tokens = preprocess(all_positive_tweets[i])\n",
    "    neg_tokens = preprocess(all_negative_tweets[i])\n",
    "    for token in pos_tokens :\n",
    "        if token in positive_dictionary :\n",
    "            positive_dictionary[token]+=1\n",
    "        else :\n",
    "            positive_dictionary[token]=1\n",
    "    for token in neg_tokens :\n",
    "        if token in negative_dictionary :\n",
    "            negative_dictionary[token]+=1\n",
    "        else :\n",
    "            negative_dictionary[token]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(tweet) :\n",
    "    if tweet in all_negative_tweets:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorize(tweets, bias) :\n",
    "    freq_table = [[]]\n",
    "    for tweet in tweets:    \n",
    "        toks = preprocess(tweet)\n",
    "        negative_score = 0\n",
    "        positive_score = 0\n",
    "        for tok in toks:\n",
    "            if tok in negative_dictionary :\n",
    "                negative_score+=negative_dictionary[tok]\n",
    "            if tok in positive_dictionary:\n",
    "                positive_score+=positive_dictionary[tok]\n",
    "        if len(toks)>0:\n",
    "            freq_table.append((np.array([bias,positive_score,negative_score])))\n",
    "    freq_table= freq_table[1:]\n",
    "    return np.array(freq_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, w):\n",
    "    sig = []\n",
    "    clip_value = 10  \n",
    "    for value in x:\n",
    "        z = np.dot(value, w) \n",
    "        z_clipped = np.clip(z, -clip_value, clip_value)\n",
    "        sig_value = 1.0 / (1 + np.exp(-z_clipped))\n",
    "        sig.append(sig_value)\n",
    "    return np.array(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, learning_rate, epochs):\n",
    "    x_train_processed = Vectorize(x_train, 1)  \n",
    "    weight = np.zeros(3)\n",
    "\n",
    "    for epoch in range(epochs+1):\n",
    "        sig = sigmoid(x_train_processed, weight)\n",
    "        loss = log_loss(y_train, sig)  \n",
    "        \n",
    "        # we use the dot product of x transpose in this situation in order to aggregate the erros\n",
    "        # meaning that we use this in order to see how much each feature is corresponding to error and adjust accordingly\n",
    "        gradient = np.dot(x_train_processed.T, (sig - y_train)) / len(y_train)\n",
    "        weight -= learning_rate * gradient\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "            print(weight)\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931471805599454\n",
      "[  0.           7.67132438 -10.83586812]\n",
      "Epoch 100, Loss: 0.05380451807778632\n",
      "[ 3.60093532e-03  8.12504401e+00 -8.73426842e+00]\n",
      "Epoch 200, Loss: 0.048563505419131035\n",
      "[ 5.40354262e-03  7.37899846e+00 -7.68161702e+00]\n",
      "Epoch 300, Loss: 0.04835419074449679\n",
      "[ 0.00690303  6.49537431 -6.76040971]\n",
      "Epoch 400, Loss: 0.04824862446026552\n",
      "[ 0.00839992  5.611057   -5.83987276]\n",
      "Epoch 500, Loss: 0.04815211106119109\n",
      "[ 0.00989522  4.72672811 -4.91935457]\n",
      "Epoch 600, Loss: 0.04785769137222982\n",
      "[ 0.01138915  3.84240224 -3.99884635]\n",
      "Epoch 700, Loss: 0.04755400561889181\n",
      "[ 0.01288289  2.95809305 -3.07834452]\n",
      "Epoch 800, Loss: 0.047261201802485886\n",
      "[ 0.01437939  2.07381784 -2.15785137]\n",
      "Epoch 900, Loss: 0.046023274571399\n",
      "[ 0.01588388  1.18960283 -1.23739229]\n",
      "Epoch 1000, Loss: 0.04247813627540391\n",
      "[ 0.01741819  0.30526287 -0.31754746]\n"
     ]
    }
   ],
   "source": [
    "weights = train(train_x,train_y,0.01,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995125\n"
     ]
    }
   ],
   "source": [
    "sigmoids = sigmoid(Vectorize(test_x,1),weights)\n",
    "correct = 0\n",
    "for i in range(len(test_x)):\n",
    "    if test_y[i] == int(sigmoids[i] > 0.5) :\n",
    "        correct+=1\n",
    "print(correct/len(sigmoids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01741819  0.30526287 -0.31754746]\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
