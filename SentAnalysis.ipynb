{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from nltk.corpus import twitter_samples\n",
    "import random\n",
    "import numpy as np\n",
    "import math as m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use predefined function ‘fileids()’ to see the content\n",
    "twitter_samples.fileids()\n",
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m@mindwiped but tomorrow lmao hehe :D\n",
      "\u001b[91m@mustntgrumble I hadn't thought of an icepack. Big doses of pain killers on board. But it is feeling very very painful already :( *whimpers*\n"
     ]
    }
   ],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "import re\n",
    "import string                              # for string operations\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "tweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \n",
    "labels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n",
    "\n",
    "train_pos  = all_positive_tweets[:4000]\n",
    "train_neg  = all_negative_tweets[:4000]\n",
    "train_x = train_pos + train_neg \n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)), axis=0).reshape(-1)\n",
    "\n",
    "\n",
    "test_pos  = all_positive_tweets[1000:]\n",
    "test_neg  = all_negative_tweets[1000:]\n",
    "test_x = test_pos + test_neg\n",
    "test_y = np.append(np.ones(len(test_pos)),np.zeros(len(test_neg)), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet) :\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    cleaned_stemmed_tweet = []\n",
    "    for token in tweet_tokens:\n",
    "        if token not in stop_words and token not in punctuation :\n",
    "            cleaned_stemmed_tweet.append(stemmer.stem(token))\n",
    "    return cleaned_stemmed_tweet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'cant', 'chang', 'peopl', 'feel', 'u', 'dnt', 'tri', 'live', 'ur', 'life', 'happi', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_processed = preprocess(all_positive_tweets[1456])\n",
    "print(tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dictionary = {}\n",
    "negative_dictionary = {}\n",
    "\n",
    "for i in range(len(all_positive_tweets)) :\n",
    "    pos_tokens = preprocess(all_positive_tweets[i])\n",
    "    neg_tokens = preprocess(all_negative_tweets[i])\n",
    "    for token in pos_tokens :\n",
    "        if token in positive_dictionary :\n",
    "            positive_dictionary[token]+=1\n",
    "        else :\n",
    "            positive_dictionary[token]=1\n",
    "    for token in neg_tokens :\n",
    "        if token in negative_dictionary :\n",
    "            negative_dictionary[token]+=1\n",
    "        else :\n",
    "            negative_dictionary[token]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(tweet) :\n",
    "    if tweet in all_negative_tweets:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorize(tweets, bias) :\n",
    "    freq_table = [[]]\n",
    "    for tweet in tweets:    \n",
    "        toks = preprocess(tweet)\n",
    "        negative_score = 0\n",
    "        positive_score = 0\n",
    "        for tok in toks:\n",
    "            if tok in negative_dictionary :\n",
    "                negative_score+=negative_dictionary[tok]\n",
    "            if tok in positive_dictionary:\n",
    "                positive_score+=positive_dictionary[tok]\n",
    "        if len(toks)>0:\n",
    "            freq_table.append((np.array([bias,positive_score,negative_score]),sentiment(tweet)))\n",
    "    freq_table= freq_table[1:]\n",
    "    return freq_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, w):\n",
    "    sig = []\n",
    "    for value in x:\n",
    "        sig.append(int((1.0 / (1 + np.double(np.exp(-np.dot(value[0], w)))))>0.5))\n",
    "    return np.array(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "weight = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train,y_train,learning_rate,epochs) :\n",
    "    x_train_processed = Vectorize(x_train,1)\n",
    "    weight = np.zeros(3)\n",
    "    for epoch in range(epochs+1):\n",
    "        sig = sigmoid(x_train_processed,weight)\n",
    "        loss = log_loss(sig, y_train, eps = 1e-15, normalize = True, sample_weight = None, labels = [0,1])\n",
    "        gradient = np.dot(np.transpose([x[0] for x in x_train_processed]), (sig - y_train)) / len(y_train)\n",
    "        weight -= learning_rate * gradient\n",
    "        if epoch  == 10000:\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "            print(weight)\n",
    "        #print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_x,train_y,0.01,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
