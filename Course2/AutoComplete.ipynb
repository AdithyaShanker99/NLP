{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adithyashanker/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import nltk               # NLP toolkit\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')    # Download the Punkt sentence tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Lyn drinks chocolate\\nJohn drinks tea\\nlyn eats chocolate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, n):\n",
    "    corpus = corpus.lower()\n",
    "    #print(corpus)\n",
    "    corpus = re.sub(r\"[^a-zA-Z0-9.?! \\n]+\", \"\", corpus)\n",
    "    #print(corpus)\n",
    "    sentences = corpus.split(\"\\n\")\n",
    "    #print(sentences)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        tokens.append(tokenized_sentence)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_prob(tokens):\n",
    "    count = {}\n",
    "    for sentence in tokens:\n",
    "        for token in sentence:\n",
    "            count[token] = count.get(token,0)+1\n",
    "    return count\n",
    "\n",
    "counts = count_prob(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = preprocess(corpus, 2)\n",
    "counts = count_prob(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_closed_vocab(counts, threshold=2):\n",
    "    vocab = []\n",
    "    for k,v in counts.items():\n",
    "        if v >=threshold:\n",
    "            vocab.append(k)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lyn', 'drinks', 'chocolate']\n"
     ]
    }
   ],
   "source": [
    "vocab = create_closed_vocab(counts)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lyn', 'drinks', 'chocolate'], ['john', 'drinks', 'tea'], ['lyn', 'eats', 'chocolate']]\n"
     ]
    }
   ],
   "source": [
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words(train_set, closed_vocab, unkown_token=\"<UNK>\"):\n",
    "    processed_train_set = train_set.copy()\n",
    "    for sentence in processed_train_set:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in closed_vocab:\n",
    "                sentence[i] = unkown_token \n",
    "    return processed_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lyn', 'drinks', 'chocolate'], ['<UNK>', 'drinks', '<UNK>'], ['lyn', '<UNK>', 'chocolate']]\n"
     ]
    }
   ],
   "source": [
    "processed_train_set = replace_oov_words(train_set, vocab)\n",
    "print(processed_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(processed_train_set, n=3, start_token='<s>', end_token = '<e>'):\n",
    "    n_grams = {}\n",
    "    for sentence in processed_train_set:\n",
    "        sentence = [start_token] * (n-1) + sentence + [end_token]\n",
    "        for i in range(len(sentence)-(n-1)): # complete this line\n",
    "\n",
    "            # Get the n-gram from i to i+n\n",
    "            n_gram = ([sentence[i] for i in range(i,i+n)])\n",
    "            n_gram = tuple(n_gram)\n",
    "            \n",
    "            n_grams[n_gram] = n_grams.get(n_gram,0)+1\n",
    "    return n_grams\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('<s>', '<s>', 'lyn'): 2, ('<s>', 'lyn', 'drinks'): 1, ('lyn', 'drinks', 'chocolate'): 1, ('drinks', 'chocolate', '<e>'): 1, ('<s>', '<s>', '<UNK>'): 1, ('<s>', '<UNK>', 'drinks'): 1, ('<UNK>', 'drinks', '<UNK>'): 1, ('drinks', '<UNK>', '<e>'): 1, ('<s>', 'lyn', '<UNK>'): 1, ('lyn', '<UNK>', 'chocolate'): 1, ('<UNK>', 'chocolate', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "n_grams = count_n_grams(processed_train_set)\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram,0)\n",
    "            \n",
    "    denominator = previous_n_gram_count + k*vocabulary_size\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    " \n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram,0)\n",
    "            \n",
    "    numerator = n_plus1_gram_count+k\n",
    "        \n",
    "    probability = numerator/denominator\n",
    "    \n",
    "    \n",
    "    return probability    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('<s>', 'lyn'): 2, ('lyn', 'drinks'): 1, ('drinks', 'chocolate'): 1, ('chocolate', '<e>'): 2, ('<s>', '<UNK>'): 1, ('<UNK>', 'drinks'): 1, ('drinks', '<UNK>'): 1, ('<UNK>', '<e>'): 1, ('lyn', '<UNK>'): 1, ('<UNK>', 'chocolate'): 1}\n"
     ]
    }
   ],
   "source": [
    "bigrams = count_n_grams(processed_train_set, 2)\n",
    "trigrams = count_n_grams(processed_train_set, 3)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "x = estimate_probability(\"chocolate\", (\"lyn\", \"drinks\"), bigrams, trigrams, len(vocab))\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
