{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adithyashanker/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import nltk               # NLP toolkit\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')    # Download the Punkt sentence tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_corpus(filepath):\n",
    "    f = open(filepath, \"r\")\n",
    "    return (f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, n):\n",
    "    corpus = corpus.lower()\n",
    "    #print(corpus)\n",
    "    corpus = re.sub(r\"[^a-zA-Z0-9.?! \\n]+\", \"\", corpus)\n",
    "    #print(corpus)\n",
    "    sentences = corpus.split(\"\\n\")\n",
    "    #print(sentences)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        tokens.append(tokenized_sentence)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_prob(tokens):\n",
    "    count = {}\n",
    "    for sentence in tokens:\n",
    "        for token in sentence:\n",
    "            count[token] = count.get(token,0)+1\n",
    "    return count\n",
    "\n",
    "counts = count_prob(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = preprocess(corpus, 2)\n",
    "counts = count_prob(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_closed_vocab(counts, threshold=2):\n",
    "    vocab = []\n",
    "    for k,v in counts.items():\n",
    "        if v >=threshold:\n",
    "            vocab.append(k)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lyn', 'drinks', 'chocolate']\n"
     ]
    }
   ],
   "source": [
    "vocab = create_closed_vocab(counts)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lyn', 'drinks', 'chocolate'], ['john', 'drinks', 'tea'], ['lyn', 'eats', 'chocolate']]\n"
     ]
    }
   ],
   "source": [
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words(train_set, closed_vocab, unkown_token=\"<UNK>\"):\n",
    "    processed_train_set = train_set.copy()\n",
    "    for sentence in processed_train_set:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in closed_vocab:\n",
    "                sentence[i] = unkown_token \n",
    "    return processed_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lyn', 'drinks', 'chocolate'], ['<UNK>', 'drinks', '<UNK>'], ['lyn', '<UNK>', 'chocolate']]\n"
     ]
    }
   ],
   "source": [
    "processed_train_set = replace_oov_words(train_set, vocab)\n",
    "print(processed_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(processed_train_set, n=3, start_token='<s>', end_token = '<e>'):\n",
    "    n_grams = {}\n",
    "    for sentence in processed_train_set:\n",
    "        sentence = [start_token] * (n-1) + sentence + [end_token]\n",
    "        for i in range(len(sentence)-(n-1)): # complete this line\n",
    "\n",
    "            # Get the n-gram from i to i+n\n",
    "            n_gram = ([sentence[i] for i in range(i,i+n)])\n",
    "            n_gram = tuple(n_gram)\n",
    "            \n",
    "            n_grams[n_gram] = n_grams.get(n_gram,0)+1\n",
    "    return n_grams\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('<s>', '<s>', 'lyn'): 2, ('<s>', 'lyn', 'drinks'): 1, ('lyn', 'drinks', 'chocolate'): 1, ('drinks', 'chocolate', '<e>'): 1, ('<s>', '<s>', '<UNK>'): 1, ('<s>', '<UNK>', 'drinks'): 1, ('<UNK>', 'drinks', '<UNK>'): 1, ('drinks', '<UNK>', '<e>'): 1, ('<s>', 'lyn', '<UNK>'): 1, ('lyn', '<UNK>', 'chocolate'): 1, ('<UNK>', 'chocolate', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "n_grams = count_n_grams(processed_train_set)\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram,0)\n",
    "            \n",
    "    denominator = previous_n_gram_count + k*vocabulary_size\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    " \n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram,0)\n",
    "            \n",
    "    numerator = n_plus1_gram_count+k\n",
    "        \n",
    "    probability = numerator/denominator\n",
    "    \n",
    "    \n",
    "    return probability    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('<s>', 'lyn'): 2, ('lyn', 'drinks'): 1, ('drinks', 'chocolate'): 1, ('chocolate', '<e>'): 2, ('<s>', '<UNK>'): 1, ('<UNK>', 'drinks'): 1, ('drinks', '<UNK>'): 1, ('<UNK>', '<e>'): 1, ('lyn', '<UNK>'): 1, ('<UNK>', 'chocolate'): 1}\n"
     ]
    }
   ],
   "source": [
    "bigrams = count_n_grams(processed_train_set, 2)\n",
    "trigrams = count_n_grams(processed_train_set, 3)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "x = estimate_probability(\"chocolate\", (\"lyn\", \"drinks\"), bigrams, trigrams, len(vocab))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
    "\n",
    "    previous_n_gram = tuple(previous_n_gram)    \n",
    "    vocabulary = vocabulary + [end_token, unknown_token]    \n",
    "    vocabulary_size = len(vocabulary)    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)  \n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    previous_tokens = ['<s>'] * (n-1) + previous_tokens\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    \n",
    "    for word, prob in probabilities.items():\n",
    "        if start_with and not word.startswith(start_with):\n",
    "            continue\n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "    \n",
    "    return suggestion, max_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(start_word, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0):\n",
    "    tokens = [start_word]\n",
    "    while True:\n",
    "        next_word, _ = suggest_a_word(tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token, unknown_token, k)\n",
    "        if next_word == end_token:\n",
    "            break\n",
    "        tokens.append(next_word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lyn', 'drinks', 'chocolate']\n"
     ]
    }
   ],
   "source": [
    "sent = generate_sentence(\"lyn\", bigrams, trigrams, vocab)\n",
    "print(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
