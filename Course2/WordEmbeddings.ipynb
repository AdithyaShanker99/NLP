{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import punkt\n",
    "#import emoji\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'I am happy because I am learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(corpus):\n",
    "    data = re.sub(r'[,!?;-]+','.', corpus)\n",
    "    data = nltk.word_tokenize(data)\n",
    "    data = [ch.lower() for ch in data if ch.isalpha() or ch == '.']\n",
    "    return data\n",
    "\n",
    "words = preprocess(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object get_windows at 0x000002A8EAA5DAD0>\n"
     ]
    }
   ],
   "source": [
    "def get_windows(words, C):\n",
    "    windows = []\n",
    "    for i in range(C,len(words)-C):\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i-C):i] + words[(i+1):(i+C+1)]\n",
    "        yield  context_words, center_word\n",
    "\n",
    "print(get_windows(words,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus):\n",
    "    vocab = {}\n",
    "    for word in corpus:\n",
    "        vocab[word] = vocab.get(word,0)+1\n",
    "    return sorted(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(sorted_vocab, word):\n",
    "    vec = np.zeros(len(sorted_vocab))\n",
    "    #print(vec)\n",
    "    vec[sorted_vocab.index(word)] = 1\n",
    "    #print(vec)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_vector(vocab, \"because\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_context_words(context_words, vocab):\n",
    "    vec = np.mean([one_hot_vector(vocab, word) for word in context_words ], axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.   0.5  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_context_words([\"i\",\"am\",\"because\",\"i\"], vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i'],[0.25 0.25 0.   0.5  0.  ],happy, [0. 0. 1. 0. 0.]\n",
      "['am', 'happy', 'i', 'am'],[0.5  0.   0.25 0.25 0.  ],because, [0. 1. 0. 0. 0.]\n",
      "['happy', 'because', 'am', 'learning'],[0.25 0.25 0.25 0.   0.25],i, [0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "def get_windows(words, window_size):\n",
    "    for i in range(len(words) - window_size):\n",
    "        context = words[i:i + window_size]\n",
    "        target = words[i + window_size]\n",
    "        yield context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors = []\n",
    "target_vectors = []\n",
    "for context, target in get_windows(words, 2):\n",
    "    context_vectors.append(one_hot_context_words(context, vocab))\n",
    "    target_vectors.append(one_hot_vector(vocab, target))\n",
    "\n",
    "context_vectors = np.array(context_vectors)\n",
    "target_vectors = np.array(target_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=5, h1=10, out_features=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, h1)\n",
    "        self.fc2 = nn.Linear(h1, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tensors = torch.tensor(context_vectors, dtype=torch.float32)\n",
    "target_tensors = torch.tensor(target_vectors, dtype=torch.float32)\n",
    "model = Model()\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "losses = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Batch [1/0], Loss: 0.0785\n",
      "Epoch [1/100], Loss: 0.0262\n",
      "Epoch [2/100], Batch [1/0], Loss: 0.0760\n",
      "Epoch [2/100], Loss: 0.0253\n",
      "Epoch [3/100], Batch [1/0], Loss: 0.0737\n",
      "Epoch [3/100], Loss: 0.0246\n",
      "Epoch [4/100], Batch [1/0], Loss: 0.0715\n",
      "Epoch [4/100], Loss: 0.0238\n",
      "Epoch [5/100], Batch [1/0], Loss: 0.0694\n",
      "Epoch [5/100], Loss: 0.0231\n",
      "Epoch [6/100], Batch [1/0], Loss: 0.0674\n",
      "Epoch [6/100], Loss: 0.0225\n",
      "Epoch [7/100], Batch [1/0], Loss: 0.0654\n",
      "Epoch [7/100], Loss: 0.0218\n",
      "Epoch [8/100], Batch [1/0], Loss: 0.0636\n",
      "Epoch [8/100], Loss: 0.0212\n",
      "Epoch [9/100], Batch [1/0], Loss: 0.0617\n",
      "Epoch [9/100], Loss: 0.0206\n",
      "Epoch [10/100], Batch [1/0], Loss: 0.0600\n",
      "Epoch [10/100], Loss: 0.0200\n",
      "Epoch [11/100], Batch [1/0], Loss: 0.0584\n",
      "Epoch [11/100], Loss: 0.0195\n",
      "Epoch [12/100], Batch [1/0], Loss: 0.0568\n",
      "Epoch [12/100], Loss: 0.0189\n",
      "Epoch [13/100], Batch [1/0], Loss: 0.0552\n",
      "Epoch [13/100], Loss: 0.0184\n",
      "Epoch [14/100], Batch [1/0], Loss: 0.0538\n",
      "Epoch [14/100], Loss: 0.0179\n",
      "Epoch [15/100], Batch [1/0], Loss: 0.0525\n",
      "Epoch [15/100], Loss: 0.0175\n",
      "Epoch [16/100], Batch [1/0], Loss: 0.0511\n",
      "Epoch [16/100], Loss: 0.0170\n",
      "Epoch [17/100], Batch [1/0], Loss: 0.0498\n",
      "Epoch [17/100], Loss: 0.0166\n",
      "Epoch [18/100], Batch [1/0], Loss: 0.0485\n",
      "Epoch [18/100], Loss: 0.0162\n",
      "Epoch [19/100], Batch [1/0], Loss: 0.0473\n",
      "Epoch [19/100], Loss: 0.0158\n",
      "Epoch [20/100], Batch [1/0], Loss: 0.0461\n",
      "Epoch [20/100], Loss: 0.0154\n",
      "Epoch [21/100], Batch [1/0], Loss: 0.0449\n",
      "Epoch [21/100], Loss: 0.0150\n",
      "Epoch [22/100], Batch [1/0], Loss: 0.0439\n",
      "Epoch [22/100], Loss: 0.0146\n",
      "Epoch [23/100], Batch [1/0], Loss: 0.0429\n",
      "Epoch [23/100], Loss: 0.0143\n",
      "Epoch [24/100], Batch [1/0], Loss: 0.0419\n",
      "Epoch [24/100], Loss: 0.0140\n",
      "Epoch [25/100], Batch [1/0], Loss: 0.0409\n",
      "Epoch [25/100], Loss: 0.0136\n",
      "Epoch [26/100], Batch [1/0], Loss: 0.0399\n",
      "Epoch [26/100], Loss: 0.0133\n",
      "Epoch [27/100], Batch [1/0], Loss: 0.0390\n",
      "Epoch [27/100], Loss: 0.0130\n",
      "Epoch [28/100], Batch [1/0], Loss: 0.0382\n",
      "Epoch [28/100], Loss: 0.0127\n",
      "Epoch [29/100], Batch [1/0], Loss: 0.0374\n",
      "Epoch [29/100], Loss: 0.0125\n",
      "Epoch [30/100], Batch [1/0], Loss: 0.0365\n",
      "Epoch [30/100], Loss: 0.0122\n",
      "Epoch [31/100], Batch [1/0], Loss: 0.0358\n",
      "Epoch [31/100], Loss: 0.0119\n",
      "Epoch [32/100], Batch [1/0], Loss: 0.0350\n",
      "Epoch [32/100], Loss: 0.0117\n",
      "Epoch [33/100], Batch [1/0], Loss: 0.0343\n",
      "Epoch [33/100], Loss: 0.0114\n",
      "Epoch [34/100], Batch [1/0], Loss: 0.0336\n",
      "Epoch [34/100], Loss: 0.0112\n",
      "Epoch [35/100], Batch [1/0], Loss: 0.0329\n",
      "Epoch [35/100], Loss: 0.0110\n",
      "Epoch [36/100], Batch [1/0], Loss: 0.0322\n",
      "Epoch [36/100], Loss: 0.0107\n",
      "Epoch [37/100], Batch [1/0], Loss: 0.0316\n",
      "Epoch [37/100], Loss: 0.0105\n",
      "Epoch [38/100], Batch [1/0], Loss: 0.0310\n",
      "Epoch [38/100], Loss: 0.0103\n",
      "Epoch [39/100], Batch [1/0], Loss: 0.0304\n",
      "Epoch [39/100], Loss: 0.0101\n",
      "Epoch [40/100], Batch [1/0], Loss: 0.0298\n",
      "Epoch [40/100], Loss: 0.0099\n",
      "Epoch [41/100], Batch [1/0], Loss: 0.0293\n",
      "Epoch [41/100], Loss: 0.0098\n",
      "Epoch [42/100], Batch [1/0], Loss: 0.0287\n",
      "Epoch [42/100], Loss: 0.0096\n",
      "Epoch [43/100], Batch [1/0], Loss: 0.0282\n",
      "Epoch [43/100], Loss: 0.0094\n",
      "Epoch [44/100], Batch [1/0], Loss: 0.0277\n",
      "Epoch [44/100], Loss: 0.0092\n",
      "Epoch [45/100], Batch [1/0], Loss: 0.0272\n",
      "Epoch [45/100], Loss: 0.0091\n",
      "Epoch [46/100], Batch [1/0], Loss: 0.0267\n",
      "Epoch [46/100], Loss: 0.0089\n",
      "Epoch [47/100], Batch [1/0], Loss: 0.0262\n",
      "Epoch [47/100], Loss: 0.0087\n",
      "Epoch [48/100], Batch [1/0], Loss: 0.0258\n",
      "Epoch [48/100], Loss: 0.0086\n",
      "Epoch [49/100], Batch [1/0], Loss: 0.0253\n",
      "Epoch [49/100], Loss: 0.0084\n",
      "Epoch [50/100], Batch [1/0], Loss: 0.0249\n",
      "Epoch [50/100], Loss: 0.0083\n",
      "Epoch [51/100], Batch [1/0], Loss: 0.0245\n",
      "Epoch [51/100], Loss: 0.0082\n",
      "Epoch [52/100], Batch [1/0], Loss: 0.0241\n",
      "Epoch [52/100], Loss: 0.0080\n",
      "Epoch [53/100], Batch [1/0], Loss: 0.0237\n",
      "Epoch [53/100], Loss: 0.0079\n",
      "Epoch [54/100], Batch [1/0], Loss: 0.0233\n",
      "Epoch [54/100], Loss: 0.0078\n",
      "Epoch [55/100], Batch [1/0], Loss: 0.0229\n",
      "Epoch [55/100], Loss: 0.0076\n",
      "Epoch [56/100], Batch [1/0], Loss: 0.0225\n",
      "Epoch [56/100], Loss: 0.0075\n",
      "Epoch [57/100], Batch [1/0], Loss: 0.0222\n",
      "Epoch [57/100], Loss: 0.0074\n",
      "Epoch [58/100], Batch [1/0], Loss: 0.0218\n",
      "Epoch [58/100], Loss: 0.0073\n",
      "Epoch [59/100], Batch [1/0], Loss: 0.0215\n",
      "Epoch [59/100], Loss: 0.0072\n",
      "Epoch [60/100], Batch [1/0], Loss: 0.0212\n",
      "Epoch [60/100], Loss: 0.0071\n",
      "Epoch [61/100], Batch [1/0], Loss: 0.0208\n",
      "Epoch [61/100], Loss: 0.0069\n",
      "Epoch [62/100], Batch [1/0], Loss: 0.0205\n",
      "Epoch [62/100], Loss: 0.0068\n",
      "Epoch [63/100], Batch [1/0], Loss: 0.0202\n",
      "Epoch [63/100], Loss: 0.0067\n",
      "Epoch [64/100], Batch [1/0], Loss: 0.0199\n",
      "Epoch [64/100], Loss: 0.0066\n",
      "Epoch [65/100], Batch [1/0], Loss: 0.0196\n",
      "Epoch [65/100], Loss: 0.0065\n",
      "Epoch [66/100], Batch [1/0], Loss: 0.0193\n",
      "Epoch [66/100], Loss: 0.0064\n",
      "Epoch [67/100], Batch [1/0], Loss: 0.0191\n",
      "Epoch [67/100], Loss: 0.0064\n",
      "Epoch [68/100], Batch [1/0], Loss: 0.0188\n",
      "Epoch [68/100], Loss: 0.0063\n",
      "Epoch [69/100], Batch [1/0], Loss: 0.0185\n",
      "Epoch [69/100], Loss: 0.0062\n",
      "Epoch [70/100], Batch [1/0], Loss: 0.0183\n",
      "Epoch [70/100], Loss: 0.0061\n",
      "Epoch [71/100], Batch [1/0], Loss: 0.0180\n",
      "Epoch [71/100], Loss: 0.0060\n",
      "Epoch [72/100], Batch [1/0], Loss: 0.0178\n",
      "Epoch [72/100], Loss: 0.0059\n",
      "Epoch [73/100], Batch [1/0], Loss: 0.0175\n",
      "Epoch [73/100], Loss: 0.0058\n",
      "Epoch [74/100], Batch [1/0], Loss: 0.0173\n",
      "Epoch [74/100], Loss: 0.0058\n",
      "Epoch [75/100], Batch [1/0], Loss: 0.0171\n",
      "Epoch [75/100], Loss: 0.0057\n",
      "Epoch [76/100], Batch [1/0], Loss: 0.0168\n",
      "Epoch [76/100], Loss: 0.0056\n",
      "Epoch [77/100], Batch [1/0], Loss: 0.0166\n",
      "Epoch [77/100], Loss: 0.0055\n",
      "Epoch [78/100], Batch [1/0], Loss: 0.0164\n",
      "Epoch [78/100], Loss: 0.0055\n",
      "Epoch [79/100], Batch [1/0], Loss: 0.0162\n",
      "Epoch [79/100], Loss: 0.0054\n",
      "Epoch [80/100], Batch [1/0], Loss: 0.0160\n",
      "Epoch [80/100], Loss: 0.0053\n",
      "Epoch [81/100], Batch [1/0], Loss: 0.0158\n",
      "Epoch [81/100], Loss: 0.0053\n",
      "Epoch [82/100], Batch [1/0], Loss: 0.0156\n",
      "Epoch [82/100], Loss: 0.0052\n",
      "Epoch [83/100], Batch [1/0], Loss: 0.0154\n",
      "Epoch [83/100], Loss: 0.0051\n",
      "Epoch [84/100], Batch [1/0], Loss: 0.0152\n",
      "Epoch [84/100], Loss: 0.0051\n",
      "Epoch [85/100], Batch [1/0], Loss: 0.0150\n",
      "Epoch [85/100], Loss: 0.0050\n",
      "Epoch [86/100], Batch [1/0], Loss: 0.0148\n",
      "Epoch [86/100], Loss: 0.0049\n",
      "Epoch [87/100], Batch [1/0], Loss: 0.0146\n",
      "Epoch [87/100], Loss: 0.0049\n",
      "Epoch [88/100], Batch [1/0], Loss: 0.0144\n",
      "Epoch [88/100], Loss: 0.0048\n",
      "Epoch [89/100], Batch [1/0], Loss: 0.0143\n",
      "Epoch [89/100], Loss: 0.0048\n",
      "Epoch [90/100], Batch [1/0], Loss: 0.0141\n",
      "Epoch [90/100], Loss: 0.0047\n",
      "Epoch [91/100], Batch [1/0], Loss: 0.0139\n",
      "Epoch [91/100], Loss: 0.0046\n",
      "Epoch [92/100], Batch [1/0], Loss: 0.0137\n",
      "Epoch [92/100], Loss: 0.0046\n",
      "Epoch [93/100], Batch [1/0], Loss: 0.0136\n",
      "Epoch [93/100], Loss: 0.0045\n",
      "Epoch [94/100], Batch [1/0], Loss: 0.0134\n",
      "Epoch [94/100], Loss: 0.0045\n",
      "Epoch [95/100], Batch [1/0], Loss: 0.0133\n",
      "Epoch [95/100], Loss: 0.0044\n",
      "Epoch [96/100], Batch [1/0], Loss: 0.0131\n",
      "Epoch [96/100], Loss: 0.0044\n",
      "Epoch [97/100], Batch [1/0], Loss: 0.0130\n",
      "Epoch [97/100], Loss: 0.0043\n",
      "Epoch [98/100], Batch [1/0], Loss: 0.0128\n",
      "Epoch [98/100], Loss: 0.0043\n",
      "Epoch [99/100], Batch [1/0], Loss: 0.0127\n",
      "Epoch [99/100], Loss: 0.0042\n",
      "Epoch [100/100], Batch [1/0], Loss: 0.0125\n",
      "Epoch [100/100], Loss: 0.0042\n"
     ]
    }
   ],
   "source": [
    "for j in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, len(context_tensors), batch_size):\n",
    "        context_batch = context_tensors[i:i + batch_size]\n",
    "        target_batch = target_tensors[i:i + batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context_batch)\n",
    "        loss = criterion(output, torch.argmax(target_batch, axis=1))  # Assuming target_batch is one-hot encoded\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{j+1}/{epochs}], Batch [{i//batch_size + 1}/{len(context_tensors)//batch_size}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch [{j+1}/{epochs}], Loss: {epoch_loss / len(context_tensors):.4f}')\n",
    "    losses.append(epoch_loss / len(context_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02698524296283722\n"
     ]
    }
   ],
   "source": [
    "print(losses[len(losses)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor([0.5,  0.0 ,  0.25, 0.25, 0.0  ], dtype=torch.float32)\n",
    "idx = int(torch.argmax(model(test)))\n",
    "print(vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
