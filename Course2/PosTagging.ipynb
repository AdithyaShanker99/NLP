{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_post(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"WSJ_02-21.pos\"\n",
    "lines = read_post(filename)\n",
    "\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "    \n",
    "print(\"A sample of the test corpus\")\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [line.split('\\t')[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(int)\n",
    "for word in words:\n",
    "    freq[word] += 1\n",
    "\n",
    "print(freq[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [k for k,v in freq.items() if k!=\" \" and k!=\"\\n\" and v>1]\n",
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab_dic = {}\n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(vocab)): \n",
    "    vocab_dic[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sort()\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "def tag_word(tok):\n",
    "    \"\"\"\n",
    "    Assign unknown word tokens\n",
    "    \"\"\"\n",
    "    # Digits\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Punctuation\n",
    "    elif any(char in punctuation for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Upper-case\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Nouns\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Verbs\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Adjectives\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Adverbs\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    return \"--unk--\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocab, data_fp):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    \"\"\"\n",
    "    orig = []\n",
    "    prep = []\n",
    "\n",
    "    # Read data\n",
    "    with open(data_fp, \"r\") as data_file:\n",
    "\n",
    "        for cnt, word in enumerate(data_file):\n",
    "\n",
    "            # End of sentence\n",
    "            if not word.split():\n",
    "                orig.append(word.strip())\n",
    "                word = \"--n--\"\n",
    "                prep.append(word)\n",
    "                continue\n",
    "\n",
    "            # Handle unknown words\n",
    "            elif word.strip() not in vocab:\n",
    "                orig.append(word.strip())\n",
    "                word = tag_word(word)\n",
    "                prep.append(word)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                orig.append(word.strip())\n",
    "                prep.append(word.strip())\n",
    "\n",
    "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
    "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
    "\n",
    "    return orig, prep\n",
    "# TO be deleted later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, prep = preprocess(vocab, \"test.words\")     \n",
    "print(prep[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tag_word(\"scrutinize\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab): \n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "        return word, tag\n",
    "    else:\n",
    "        word, tag = line.split()\n",
    "        if word not in vocab: \n",
    "            # Handle unknown words\n",
    "            word = tag_word(word)\n",
    "        return word, tag\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pair_dictionary(lines, corpus):\n",
    "    i = 0\n",
    "    tag_trans = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    emissions = defaultdict(int)\n",
    "    prev_tag = \"--s--\"\n",
    "    for line in lines:\n",
    "        word, next_tag = get_word_tag(line, corpus)\n",
    "        tag_trans[(prev_tag, next_tag)] +=1\n",
    "        tag_counts[next_tag] +=1\n",
    "        emissions[(next_tag, word)] +=1\n",
    "        prev_tag = next_tag\n",
    "        i+=1\n",
    "        if i%50000==0:\n",
    "            print(i)\n",
    "     \n",
    "    return tag_trans, tag_counts, emissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_tags(tag_transitions, counts, alpha):\n",
    "    prob = np.zeros((len(counts), len(counts)))\n",
    "    counts_sorted = sorted(counts.keys())\n",
    "    for i in range(len(counts)):\n",
    "        for j in range(len(counts)):\n",
    "            # if counts_sorted[i] == \"--s--\":\n",
    "            #     print(i)\n",
    "            # print(counts_sorted[j])\n",
    "            # print(tag_transitions[(counts_sorted[i], counts_sorted[j])])\n",
    "            # print(\"\\n-----------------\\n\\n\\n\")\n",
    "            # print(counts[counts_sorted[j]]+alpha*len(counts))\n",
    "            prob[i][j] = (tag_transitions[(counts_sorted[i], counts_sorted[j])] + alpha)/(counts[counts_sorted[i]]+alpha*len(counts))\n",
    "            \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_trans, tag_counts, emissions = create_pair_dictionary(lines, prep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = calculate_probabilities_tags(tag_trans, tag_counts, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_emissions(emissions, counts, alpha, corpus):\n",
    "    prob = np.zeros((len(counts), len(corpus)))\n",
    "    counts_sorted = sorted(counts.keys())\n",
    "    for i in range(len(counts)):\n",
    "        for j in range(len(corpus)):\n",
    "            # print(counts_sorted[i])\n",
    "            # print(vocab[j])\n",
    "            # print(emissions[(counts_sorted[i], vocab[j])])\n",
    "            # print(\"\\n-----------------\\n\\n\\n\")\n",
    "            # print(counts[counts_sorted[j]]+alpha*len(counts))\n",
    "            prob[i][j] = (emissions[(counts_sorted[i], corpus[j])] + alpha)/(counts[counts_sorted[i]]+alpha*len(corpus))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_emissions = calculate_probabilities_emissions(emissions, tag_counts, 0.001, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob_emissions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_initialization(prob_emissions, probs, corpus):\n",
    "\n",
    "    # prob_emissions = np.array(prob_emissions)\n",
    "    # probs = np.array(probs)\n",
    "    # probs_pi = np.vstack([probs[6]]*len(probs))\n",
    "    # #print(probs_pi)\n",
    "    # c = np.matmul(probs_pi, prob_emissions)\n",
    "    # #print(c)\n",
    "    # return c\n",
    "\n",
    "    c = np.zeros((len(probs), len(corpus)))\n",
    "    for i in range(len(probs)):\n",
    "        pi = probs[6][i]\n",
    "        for j in range(len(corpus)):\n",
    "            emission_prob = prob_emissions[i][vocab_dic[corpus[0]]]\n",
    "            c[i][j] = np.log(pi)+np.log(emission_prob)\n",
    "    d = np.zeros((len(probs), len(corpus)), dtype=int)\n",
    "\n",
    "    return c,d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = viterbi_initialization(prob_emissions, probs, prep)\n",
    "print(c[0][0])\n",
    "print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def viterbi_forward(trans_mat, emission_prob, c, d,vocab, corpus):\n",
    "    n_states = len(c)\n",
    "    n_obs = len(corpus)\n",
    "\n",
    "    for j in range(1, n_obs):\n",
    "        for i in range(n_states):\n",
    "            max_prob = -np.inf\n",
    "            for k in range(n_states):\n",
    "                prob = c[k][j-1] + np.log(trans_mat[k][i]) + np.log(emission_prob[i][vocab.get(corpus[j], 0)])\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "            c[i][j] = max_prob\n",
    "                \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(vocab))\n",
    "forward = viterbi_forward(probs,prob_emissions, c,d,vocab_dic, prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forward[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
