{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adithyashanker/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd # Library for Dataframes \n",
    "\n",
    "import pickle # Python object serialization library. Not secure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
    "    Check out the files this function takes in your workspace.\n",
    "    \"\"\"\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_embeddings = pickle.load( open( \"DataSets/en_embeddings.p\", \"rb\" ) )\n",
    "fr_word_embeddings = pickle.load( open( \"DataSets/fr_embeddings.p\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sy/2jzm18gs78s48trcqgthb1xh0000gn/T/ipykernel_44886/683761547.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "/var/folders/sy/2jzm18gs78s48trcqgthb1xh0000gn/T/ipykernel_44886/683761547.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sy/2jzm18gs78s48trcqgthb1xh0000gn/T/ipykernel_44886/683761547.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "/var/folders/sy/2jzm18gs78s48trcqgthb1xh0000gn/T/ipykernel_44886/683761547.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    }
   ],
   "source": [
    "en_fr_train = get_dict('./DataSets/en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('./DataSets/en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Matrices(train_set, emb1, emb2):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for key in train_set:\n",
    "        if key in emb1 and train_set[key] in emb2:\n",
    "            X.append(emb1[key])\n",
    "            Y.append(emb2[train_set[key]])\n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4932, 300)\n",
      "(4932, 300)\n"
     ]
    }
   ],
   "source": [
    "X,Y = create_Matrices(en_fr_train, en_word_embeddings, fr_word_embeddings)\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))\n",
    "\n",
    "# print(X)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, R):\n",
    "    \n",
    "    m = np.shape(X)[0]\n",
    "    diff = np.matmul(X,R)-Y\n",
    "    diff_squared = np.square(diff)\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "    loss = sum_diff_squared/m\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X,Y,R,m):\n",
    "    return (2/m)*( np.matmul(X.T ,(np.matmul(X,R)-Y) )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
      "  0.04980469 -0.00952148  0.22070312 -0.12597656  0.08056641 -0.5859375\n",
      " -0.00445557 -0.296875   -0.01312256 -0.08349609  0.05053711  0.15136719\n",
      " -0.44921875 -0.0135498   0.21484375 -0.14746094  0.22460938 -0.125\n",
      " -0.09716797  0.24902344 -0.2890625   0.36523438  0.41210938 -0.0859375\n",
      " -0.07861328 -0.19726562 -0.09082031 -0.14160156 -0.10253906  0.13085938\n",
      " -0.00346375  0.07226562  0.04418945  0.34570312  0.07470703 -0.11230469\n",
      "  0.06738281  0.11230469  0.01977539 -0.12353516  0.20996094 -0.07226562\n",
      " -0.02783203  0.05541992 -0.33398438  0.08544922  0.34375     0.13964844\n",
      "  0.04931641 -0.13476562  0.16308594 -0.37304688  0.39648438  0.10693359\n",
      "  0.22167969  0.21289062 -0.08984375  0.20703125  0.08935547 -0.08251953\n",
      "  0.05957031  0.10205078 -0.19238281 -0.09082031  0.4921875   0.03955078\n",
      " -0.07080078 -0.0019989  -0.23046875  0.25585938  0.08984375 -0.10644531\n",
      "  0.00105286 -0.05883789  0.05102539 -0.0291748   0.19335938 -0.14160156\n",
      " -0.33398438  0.08154297 -0.27539062  0.10058594 -0.10449219 -0.12353516\n",
      " -0.140625    0.03491211 -0.11767578 -0.1796875  -0.21484375 -0.23828125\n",
      "  0.08447266 -0.07519531 -0.25976562 -0.21289062 -0.22363281 -0.09716797\n",
      "  0.11572266  0.15429688  0.07373047 -0.27539062  0.14257812 -0.0201416\n",
      "  0.10009766 -0.19042969 -0.09375     0.14160156  0.17089844  0.3125\n",
      " -0.16699219 -0.08691406 -0.05004883 -0.24902344 -0.20800781 -0.09423828\n",
      " -0.12255859 -0.09472656 -0.390625   -0.06640625 -0.31640625  0.10986328\n",
      " -0.00156403  0.04345703  0.15625    -0.18945312 -0.03491211  0.03393555\n",
      " -0.14453125  0.01611328 -0.14160156 -0.02392578  0.01501465  0.07568359\n",
      "  0.10742188  0.12695312  0.10693359 -0.01184082 -0.24023438  0.0291748\n",
      "  0.16210938  0.19921875 -0.28125     0.16699219 -0.11621094 -0.25585938\n",
      "  0.38671875 -0.06640625 -0.4609375  -0.06176758 -0.14453125 -0.11621094\n",
      "  0.05688477  0.03588867 -0.10693359  0.18847656 -0.16699219 -0.01794434\n",
      "  0.10986328 -0.12353516 -0.16308594 -0.14453125  0.12890625  0.11523438\n",
      "  0.13671875  0.05688477 -0.08105469 -0.06152344 -0.06689453  0.27929688\n",
      " -0.19628906  0.07226562  0.12304688 -0.20996094 -0.22070312  0.21386719\n",
      " -0.1484375  -0.05932617  0.05224609  0.06445312 -0.02636719  0.13183594\n",
      "  0.19433594  0.27148438  0.18652344  0.140625    0.06542969 -0.14453125\n",
      "  0.05029297  0.08837891  0.12255859  0.26757812  0.0534668  -0.32226562\n",
      " -0.20703125  0.18164062  0.04418945 -0.22167969 -0.13769531 -0.04174805\n",
      " -0.00286865  0.04077148  0.07275391 -0.08300781  0.08398438 -0.3359375\n",
      " -0.40039062  0.01757812 -0.18652344 -0.0480957  -0.19140625  0.10107422\n",
      "  0.09277344 -0.30664062 -0.19921875 -0.0168457   0.12207031  0.14648438\n",
      " -0.12890625 -0.23535156 -0.05371094 -0.06640625  0.06884766 -0.03637695\n",
      "  0.2109375  -0.06005859  0.19335938  0.05151367 -0.05322266  0.02893066\n",
      " -0.27539062  0.08447266  0.328125    0.01818848  0.01495361  0.04711914\n",
      "  0.37695312 -0.21875    -0.03393555  0.01116943  0.36914062  0.02160645\n",
      "  0.03466797  0.07275391  0.16015625 -0.16503906 -0.296875    0.15039062\n",
      " -0.29101562  0.13964844  0.00448608  0.171875   -0.21972656  0.09326172\n",
      " -0.19042969  0.01599121 -0.09228516  0.15722656 -0.14160156 -0.0534668\n",
      "  0.03613281  0.23632812 -0.15136719 -0.00689697 -0.27148438 -0.07128906\n",
      " -0.16503906  0.18457031 -0.08398438  0.18554688  0.11669922  0.02758789\n",
      " -0.04760742  0.17871094  0.06542969 -0.03540039  0.22949219  0.02697754\n",
      " -0.09765625  0.26953125  0.08349609 -0.13085938 -0.10107422 -0.00738525\n",
      "  0.07128906  0.14941406 -0.20605469  0.18066406 -0.15820312  0.05932617\n",
      "  0.28710938 -0.04663086  0.15136719  0.4921875  -0.27539062  0.05615234]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(en_word_embeddings[\"cat\"]))\n",
    "print(en_word_embeddings['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(train_set,emb1,emb2, alpha=0.8, epochs = 400):\n",
    "    X,Y = np.array(create_Matrices(train_set, emb1, emb2))\n",
    "    m = np.shape(X)[0]\n",
    "    print(m)\n",
    "    R = np.random.rand(np.shape(X)[1], np.shape(X)[1])\n",
    "    for epoch in range(epochs):\n",
    "        grad = calculate_gradient(X,Y,R,m)\n",
    "        R -= alpha*grad\n",
    "        if epoch%100 == 0:\n",
    "            print(loss(X,Y,R))\n",
    "    return R\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4932\n",
      "688.9905564261976\n",
      "4.316148827970291\n",
      "0.8252619111004823\n",
      "0.598188990075977\n"
     ]
    }
   ],
   "source": [
    "R = Train(en_fr_train, en_word_embeddings, fr_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01300136 -0.0138153  -0.00036254 ... -0.00088329 -0.0060087\n",
      "  -0.00194302]\n",
      " [ 0.01594322  0.00102746  0.00195759 ...  0.02410531  0.00223486\n",
      "   0.00077226]\n",
      " [ 0.00837645  0.00302113  0.01759573 ...  0.00082764 -0.00240765\n",
      "   0.00804419]\n",
      " ...\n",
      " [ 0.00130421  0.0057092  -0.01679318 ... -0.00774603  0.00539664\n",
      "   0.00551702]\n",
      " [ 0.00525873 -0.0043057   0.00161557 ... -0.00278822  0.00317019\n",
      "   0.00208333]\n",
      " [ 0.0025919  -0.01566492 -0.0017021  ... -0.01819667  0.00869492\n",
      "  -0.00786954]]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def side_of_plane(normal, vec):\n",
    "    side = np.sign(np.dot(normal, vec.T))\n",
    "    if side < 0 :\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "print(side_of_plane(np.array([1,1]), np.array([-2,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiplane_hashing(planes, vec):\n",
    "    value = 0\n",
    "    for i in range(len(planes)):\n",
    "        value += (2 ** i) * side_of_plane(planes[i], vec)\n",
    "    return value\n",
    "\n",
    "P1 = np.array([[1, 1]])   # First plane 2D\n",
    "P2 = np.array([[-1, 1]])  # Second plane 2D\n",
    "P3 = np.array([[-1, -1]]) # Third plane 2D\n",
    "P_l = [P1, P2, P3]  # List of arrays. It is the multi plane\n",
    "\n",
    "# Vector to search\n",
    "v = np.array([[2, 2]])\n",
    "\n",
    "multiplane_hashing(P_l, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hash_embeddings(embeddings):\n",
    "    hashmap = {}\n",
    "    planes = np.random.normal(size=(10,300))\n",
    "    for key in embeddings:\n",
    "        vec = embeddings[key]\n",
    "        hash = multiplane_hashing(planes, vec)\n",
    "        if hash not in hashmap:\n",
    "            hashmap[hash] = []\n",
    "        hashmap[hash].append((key,vec))\n",
    "    return hashmap,planes\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance(Train_features, Target_features) :\n",
    "    distance = np.sqrt(np.sum((Train_features - Target_features) ** 2))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(v1, v2, ):\n",
    "    return (v1@v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosineSimilarity):\n",
    "    similarity_l = []\n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(row,v)\n",
    "        similarity_l.append(cos_similarity)\n",
    "\n",
    "    sorted_ids = np.argsort(similarity_l)[::-1]\n",
    "    \n",
    "    k_idx = sorted_ids[:k]\n",
    "\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5584144645340751\n"
     ]
    }
   ],
   "source": [
    "X_test,Y_test = create_Matrices(en_fr_test, en_word_embeddings, fr_word_embeddings)\n",
    "\n",
    "def test(X_test, weights, Y_test):\n",
    "    prediction_matrix = np.matmul(X_test,weights)\n",
    "    Accuracy = 0\n",
    "    for i in range(len(X_test)) :\n",
    "        if nearest_neighbor(prediction_matrix[i], Y_test) == i:\n",
    "            Accuracy+=1\n",
    "    return Accuracy/len(X_test)\n",
    "                \n",
    "\n",
    "print(test(X_test, R, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(word ,X, emb = en_word_embeddings):\n",
    "    word_emb = emb[word]\n",
    "    for i in range(len(X)) :\n",
    "        if X[i] == word_emb :\n",
    "            return i\n",
    "    return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
